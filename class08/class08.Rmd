---
title: 'Class 8: clustering and PCA'
author: "Madison Hale"
date: "2/5/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K-means clustering

```{r}
# Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x = tmp, y = rev(tmp))

plot(x)

```

Use the kmeans() function setting k to 2 and nstart=20
Inspect/print the results
Q. How many points are in each cluster?
Q. What ‘component’ of your result object details
 - cluster size?
 - cluster assignment/membership?
 - cluster center?
Plot x colored by the kmeans cluster assignment and
 add cluster centers as blue points
 
```{r}

km <- kmeans(x, centers = 2, nstart = 20)
#View(km)

#used ?kmeans in console 
```
```{r}
#prints km
km
```

```{r}
km$size 
#tells us there are 30 points in each cluster
```

Cluster membership assignment vector (which group data points lie in)
```{r}
km$cluster
```

Plots points, colors differ based on cluster
```{r}
plot(x, col = km$cluster)
points(km$centers, col = "green", pch = 15, cex = 2)
```

```{r}
km$totss
```

## Hierarchical clustering in R

```{r}
# First we need to calculate point (dis)similarity
# as the Euclidean distance between observations
dist_matrix <- dist(x)


# The hclust() function returns a hierarchical
# clustering model
hc <- hclust(d = dist_matrix)


# the print method is not so useful here
hc 

```

```{r}
#viewing the data
#View(hc)
#View(x)
#View(as.matrix(dist_matrix))
dim(as.matrix(dist_matrix))
```

```{r}
#draws a dendrogram...tall "goal post means points are far apart"
plot(hc)
abline(h = 6, col = "red")

#two ways to use cutree()
cutree(hc, h = 6) # Cut by height h
#or...
cutree(hc, k = 2) # Cut into k grps
```

## Linkage (linking clusters) in R

```{r}
# Using different hierarchical clustering methods
d <- dist_matrix
hc.complete <- hclust(d, method = "complete")
plot(hc.complete)

hc.average <- hclust(d, method = "average")
plot(hc.average)

hc.single <- hclust(d, method = "single")
plot(hc.single)

#want results to be independent of which linkage method is used
```

## Trying this out!

```{r}
# Step 1. Generate some example data for clustering
x <- rbind(
 matrix(rnorm(100, mean = 0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
 rnorm(50, mean = 0, sd = 0.3)), ncol = 2))

colnames(x) <- c("x", "y")

```

```{r}
# Step 2. Plot the data without clustering
plot(x)
```
```{r}
# Step 3. Generate colors for known clusters
# (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each = 50) )
plot(x, col = col)
```

Use the dist(), hclust(), plot() and cutree() functions to return 2 and 3 clusters

```{r}
d <- dist(x)
hc <- hclust(d)
#dim(as.matrix(hc))
plot(hc)
```

```{r}
plot(hc)
abline( h = 2.5, col = "red")
abline( h = 2, col = "blue")
```
```{r}
gp2 <- cutree(hc, k = 2)
gp3 <- cutree(hc, k = 3)
plot(x, col = gp3)
```

The two functions we are learning:
  kmeans( x, centers = #, nstart = #)
  hclust( dist(x))
  

PCA = Principal Component Analysis...helps to analyze spread of data
  -> PCA converts the correlations (or lack there of) among all cells into a representation we can more readily interpret (e.g. a 2D graph!)
  
NOTE: The PCs (i.e. new plot axis) are ranked by their importance...PC1 is more important than PC2 which in turn is more important than PC3 etc. The PCs (i.e. new plot axis) are
ranked by the amount of variance in the original data that they “capture”.
  ...in R, PCA is used with  prcomp() 
  
## Using PCA

```{r}
## downloaded file from class website
mydata <- read.csv("https://tinyurl.com/expression-CSV", row.names = 1)

head(mydata)
```
```{r}
#head(t(mydata))
pca <-  prcomp(t(mydata), scale = TRUE)
attributes(pca) 

```

```{r}
## A basic PC1 vs PC2 2-D plot 
plot(pca$x[,1], pca$x[,2])
```
```{r}
## Variance captured per PC
pca.var <- pca$sdev^2 

## Precent variance is often more informative to look at
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1) 

pca.var.per
```
```{r}
barplot(pca.var.per, main = "Scree Plot", xlab = "Principal Component", ylab = "Percent Variation")
```

```{r}
#improving the plot
## A vector of colors for wt and ko samples
colvec <- colnames(mydata)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"
plot(pca$x[,1], pca$x[,2], col = colvec, pch = 16,
 xlab = paste0("PC1 (", pca.var.per[1], "%)"),
 ylab = paste0("PC2 (", pca.var.per[2], "%)")) 

```
```{r}
#and even better

plot(pca$x[,1], pca$x[,2], col = colvec, pch = 16, xlab = paste0("PC1 (", pca.var.per[1], "%)"), ylab = paste0("PC2 (", pca.var.per[2], "%)"))
## Click to identify which sample is which
identify(pca$x[,1], pca$x[,2], labels = colnames(mydata))
## Press ESC to exit… 
```

## Perform a PCA on the UK foods dataset

Input: read, View/head
PCA: prcomp
Plots: PCA plot, scree plot, loadings plot.
  
```{r}
x <- read.csv("UK_foods.csv")
dim(x)
```
#Side-Note: Never leave a View() function call uncommented in your Rmarkdown document...this is intended for interactive use and will stop the Knit rendering process when you go to generate HTML, PDF, MD etc. reports.
  
```{r}
#View(x[6,])

#fixing: set rownames() to the first column and then remove troublesome first column (with the -1 column index)

rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```
```{r}
#better way to set proper row_names (reading data properly in the first place):

x <- read.csv("UK_foods.csv", row.names = 1)
head(x)
```

Trying to spot differences in data with some plots
```{r}
barplot(as.matrix(x), beside = T, col = rainbow(nrow(x)))
barplot(as.matrix(x), beside = F, col = rainbow(nrow(x)))
pairs(x, col = rainbow(10), pch = 16)

#these plots are all challenging to interpret
```

PCA to the rescue!

```{r}
# prcomp() expects the observations to be rows and the variables to be columns therefore we need to first transpose our data.frame matrix with the t() transpose function

# Use the prcomp() PCA function 
pca <- prcomp( t(x) )
summary(pca)

# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab = "PC1", ylab = "PC2", xlim = c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col = c("red", "yellow", "orange", "green"))

```

### Examining the "loadings"

This will help us determine how the origional variables (dimensions) contribute to our new PCs

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar = c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las = 2 )
```
PCAs reduce dimensionality so we can better visualize/interpret data and observe outliers.


  
  
  
  
  
  
  
  
  