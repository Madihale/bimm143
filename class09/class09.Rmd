---
title: 'Class 9: Unsupervised Learning Mini-Project'
author: "Madison Hale"
date: "2/7/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Human Breast Cancer Cells

```{r}
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data)
head(wisc.df)
#View(wisc.df)
```
How many samples (patients) are in this data set?
```{r}
nrow(wisc.df)
colnames(wisc.df)
```

use as.matrix() to convert the other features (i.e. columns) of the data (in columns 3 through 32) to a matrix. Store this in a variable called wisc.data.

```{r}
wisc.data <- as.matrix(wisc.df[,3:32])

#adding rownames to our new matrix of data
row.names(wisc.data) <- wisc.df$id

head(wisc.data)
```
```{r}
#checking the dimensions
dim(wisc.data)
```

Finally, setup a separate new vector called diagnosis to be 1 if a diagnosis is malignant ("M") and 0 otherwise. Note that R coerces TRUE to 1 and FALSE to 0.


How many cancer (M) and non cancer (B) samples do we have in our data-set?
```{r}
# Create diagnosis vector by completing the missing code
table(wisc.df$diagnosis)
```

```{r}
diagnosis <- as.numeric(wisc.df$diagnosis)
#tmp <- rep(0, nrow(wisc.df))
diagnosis <- as.numeric(wisc.df$diagnosis == "M")
```

Q1. How many observations are in this dataset?
```{r}
nrow(wisc.data)
```

Q2. How many variables/features in the data are suffixed with _mean?

```{r}
#?grep
inds <- grep("_mean", colnames(wisc.data))
length(inds)
#checking with: colnames(wisc.data)
```

Q3. How many of the observations have a malignant diagnosis?

```{r}
sum(diagnosis)
```

##set-up finished...now time for PCA!

Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to determine if the data should be scaled. Use the colMeans() and apply() functions 

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale = TRUE)
summary(wisc.pr)
```

Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
  -> 44.27%

Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
  -> 3

Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
  -> 7
  
##Plots of our results:

First a standard biplot

```{r}
#not helpful, we have too much data!
biplot(wisc.pr)
```

Need to make our own PCA plot

```{r}
#x is how we access the PC data
#plot(wisc.pr$x[,1], wisc.pr$x[,2], col = wisc.df$diagnosis ,  xlab = "PC1", ylab = "PC2")
#or
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis + 1 ,  xlab = "PC1", ylab = "PC2")

#the +1 changes the color from white and black -> red and black by changing the data from 0's and 1's to 1's and 2's (so there is no white colored points)
```

### Variance captured in each PC

This info is in the $sdev component of our PCA result
```{r}
# Variance explained by each principal component: pve
variance <- wisc.pr$sdev^2
#rounded to make it easier to read
pve <- round((variance/sum(variance)) * 100, 1)
```

```{r}
plot(pve, type = "o")
```

```{r}
barplot(pve)
```

Made the plot prettier:

```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg = paste0("PC",1:length(pve)), las = 2, axes = FALSE)
axis(2, at = pve, labels = round(pve,2))
```


##Hierarchical Clustering

For hierarchical clustering we need a few things:
1. Distance matrix **dist()** function.
2. The **hclust()** function.
3. Use the **cutree()** function to find cluster membership vector.

```{r}
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)
```

```{r}
round( apply(wisc.data, 2, sd), 1)
```

```{r}
round( apply(data.scaled, 2, sd), 1) #means that columns are equally weighted, important!!
```

Looks like we do need scalling!


```{r}
#Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to  data.dist.
data.dist <- dist(data.scaled)

```

```{r}
#Create a hierarchical clustering model using complete linkage
wisc.hclust <- hclust(data.dist, method = "complete")
plot(wisc.hclust) #ugly plot!
```

(side note!) New functions:
grep("_mean", x)
prcomp(x, scale = TRUE)
hclust(dist(x), method - ?)

# (section 6) Cluster in PCA space...

For clustering we need?
1. Distance matrix
2. Clustering function (hclust)
3. Cutree

This was our PCA result of PC1 vs PC2
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis + 1 ,  xlab = "PC1", ylab = "PC2")
```

```{r}
pc.dist <- dist( wisc.pr$x[,1:2] )
pc.hclust <-  hclust(pc.dist, method = "ward.D2")
plot(pc.hclust)
#much better graph...2 distinguishable groups present!
```

```{r}
#using cutree
grps3 <- cutree(pc.hclust, k = 3)
table(grps3)
```

```{r}
table(grps3, diagnosis)
```

```{r}
grps2 <- cutree(pc.hclust, k = 2)
table(grps2)
```
```{r}
table(grps2, diagnosis)
```
```{r}
plot(wisc.pr$x[,1:2], col = grps2)
plot(wisc.pr$x[,1:2], col=diagnosis+3)
```
 installing rlg with install.packages("rgl")
 
```{r}
#update: did not do this optional step because of installation difficulties
#using rgl
#library(rgl)
#plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=diagnosis+1)
```
 
Section 7: use the predict() function that will take our PCA model from before and new cancer cell data and project that data onto our PCA space.

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```
```{r}
plot(wisc.pr$x[,1:2], col = grps2)
points(npc[,1], npc[,2], col = "purple", pch = 16, cex = 3)
```

**can change .Rmd to .html in web browser to see code from all classes :)

